{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "Welcome to exercise one of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise you\u2019ll apply the basics of functional and parallel programming. \n\nLet\u2019s start with a simple example. Let\u2019s consider you have a list of integers.\n\nLet\u2019s find out what the size of this list is.\n\nNote that we already provide an RDD object, so please have a look at the RDD API in order to find out what function to use:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n\nThe following link contains additional documentation:\nhttps://spark.apache.org/docs/latest/rdd-programming-guide.html\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20191117114433-0000\nKERNEL_ID = b91a08f4-5537-4125-b52f-a0e3c43ff8c8\n"
                },
                {
                    "data": {
                        "text/plain": "'2.3.3'"
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "sc.version"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "rdd = sc.parallelize(range(100))"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "100"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# please replace $$ with the correct characters\nrdd.count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "You should see \"100\" as answer. Now we want to know the sum of all elements. Please again, have a look at the API documentation and complete the code below in order to get the sum."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "4950"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.sum()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "You should get \"4950\" as answer."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}